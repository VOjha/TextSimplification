%
% File naaclhlt2015.tex
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fix saccades
% Fix tone, metadiscussion, length
% Results section!!!!
% Finish intro
% Finish summary/future plans
% Upload images to Github
% Anonymize
% Update Github
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2015}
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Eye-tracking without the eye-tracker\Thanks{Acknowledgements and thanks go here.}}

\author{Vidushi Ojha, Adam Shaw, Julie Medero\\
	Harvey Mudd College\\
	340 E. Foothill Boulevard\\
	Claremont, CA 91711, USA\\
	{\tt vojha@g.hmc.edu, ashaw@g.hmc.edu, julie@cs.hmc.edu}}

\date{}

\begin{document}
	\maketitle
	\begin{abstract}
		This paper details the process of creating an iPad app for the purpose of measuring eye movements during reading. The app was meant to simulate an eye-tracking experiment as closely as possible without the corresponding equipment. The aim was to replicate established results in the field of text simplification (using eye-tracking equipment) using the app. To this end, a study was designed around minimal sentence pairs, to detect fixations, regressions, and saccades. While we found that the app is capable of detecting small differences in how people process sentences, and there are some statistically significant different phrases amongst the pairs tested, most pairs showed no difference.
	\end{abstract}
	
	\section{Introduction}
	
	In 2015, the U.S. Department of Education and the National Institute of Literacy conducted a study showing that 14\% of adults in the U.S. cannot read beyond a ``basic level'' (``The U.S. Illiteracy Rate''). For the millions of adults that fall within this category, information and resources predicated on reading is difficult to access, leading to issues with daily tasks, ranging from reading the news to acquiring healthcare. A great number of these adults may be unable to read due to learning disabilities, insufficient instruction in school, or not being native English speakers. In all of these cases, it is of vital importance to these people to have available a resource to simplify reading for them, and it is in this vein that automatic text simplification projects have been undertaken.
	
	\subsection{Purpose}
	
	This paper describes a mechanism designed to conduct research in the first step of automatic text simplification: determining what it means for a text to be ``hard.'' Existing literature in this area relies on observing readers' reactions as they read a text, such as through eye-tracking equipment (Ashby \textit{et al.}). Although effective, eye-tracking studies require participants to come to specialized laboratories and put on cumbersome equipment, involving a lot of effort on the part of both the researchers and the participants. If a method can be found that is just as effective, but significantly less cumbersome, much of the research in this field can be simplified, and future research can be undertaken with ease.
	
	\subsection{Background}
	
	The research of interest to this paper has been conducted in the field of eye-tracking experiments to gauge text difficulty. Rather than create models to automatically simplify texts, this research area focuses on using human readers to identify patterns in the way they read, and use this data to estimate what characteristics of a text are easy or difficult. These patterns take on three forms in particular, based on how our eyes move around naturally when reading.
	
	\begin{itemize}[noitemsep]
		\item[1.] \textit{Fixations} refer to the short durations where the eye is fixed on a particular place or word. People usually fixate on few characters, meaning a word may require more than one fixation to be understood. [66]
		\item[2.] \textit{Regressions} occur when readers return to an earlier point in the text in order to reread it, typically due to either having missed something or requiring a reread for clarification. [17]
		\item[3.] \textit{Saccades} are the small jumps that occur between fixations, and are relatively short (10-100 ms, as opposed to fixations of typically 200-250 ms), as well as spatially random. [27]
	\end{itemize}
	
	The purpose of this experiment was to show that the app being developed was capable of detecting all three of the above behaviors, which would allow this mechanism to be useful in experiments that are like the established eye-tracking technique.
	
	Results from existing eye-tracking experiments also provide the justification for some of the particular texts tested in this experiment. In particular, the paper ``Eye movements of highly skilled and average readers: differential effects of frequency and predictability'' suggests that certain pairs of words produce distinctly different reactions in readers, and that these reactions can be monitored using the three behaviors listed above. They found a statistically significant difference between word pairs where one word was more ``predictable'' than the other, from the context of the same sentence. It is studies such as these which give us an idea of which characteristics of texts may be worth studying. Ratatatat!
	
	\section{App}
	
	In order to carry out the study, we designed an app that would allow us to test many different passages and get an idea of whether we could replicate eye-tracking results.
	
	\subsection{Implementation Details}
	
	The app developed over the course of this project was built for the iOS mobile operating system. The intended platform was an iPad, due to its screen size, frequency of use for reading, and inbuilt accelerometer. As such, the original implementation of the app was written in the Swift programming language, using the Xcode 6 IDE. During this project, both programming language and IDE were updated in beta to Swift 2.0 and Xcode 7 respectively.
	
	\subsection{Functionality}
	
	In terms of features, this application was designed specifically with this study in mind, and so its functionality is limited to precisely that which was needed for the experiment.
	
	This app was designed with minimalism in mind. For the purposes of this experiment, it was necessary to keep track of individual data sets while preserving the participant's anonymity. To this end, the first function the app supports is the ability to assign an ID number to each set of data readings. This ID number is filled into a text field by the researcher leading the study.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.4\textwidth]{splashscreen.png}
		\caption{Home screen.}
	\end{figure}
	
	From the home screen, the participant must click ``start" in order to begin the experiment. When this button is pressed, the iPad calibrates itself, meaning whatever position it is held in at that moment is treated as a zero tilt. After this, the user is presented with the reading screen, which consists of a reset button and a horizontal reading frame (a rectangular box in the center of the screen). Upon tilting, text appears inside the reading frame and scrolls through. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.4\textwidth]{readingscreen.png}
		\caption{Reading screen.}
	\end{figure}
	
	The accelerometer measures the acceleration as the user tilts the iPad, which is used to determine how fast the text scrolls. The text scrolls from right to left, and the user can make the text scroll forward by tilting the iPad to their right, as well as backward by tilting the iPad to their left. This functionality allows the user to read at their own pace, as well as affords the user the flexibility to reread passages if they so choose.
	
	Many of the settings behind the app were determined through a user study. In the study, participants were given different options for settings for the following parameters:
	\begin{itemize}[noitemsep, nolistsep]
		\item Tilt sensitivity
		\item Reading frame size
		\item Font size
		\item Font choice
		\item Tilt direction
	\end{itemize}
	
	Thus, the above settings were uniform across participants for this text comparison study. Data is collected by the accelerometer built into the iPad, which reads the device's acceleration every 0.2 seconds, and associates this value with the time elapsed and the text visible on the screen at that time.
	
	\section{Experiment}
	\label{sec:exp}
	
	The app was specifically designed for this specific study, which was carried out over the course of three days on the campus of Harvey Mudd College.
	
	\subsection{Participants}
	
	The participants for this study were recruited from amongst summer researchers at Harvey Mudd via email signups. They were told that they would be expected to read a number of passages and summarize them for the researcher leading the study, and that they would be compensated \$5 for their time. Through this recruiting process, the study had 22 participants between the ages of 18 and 22, 10 of whom were female, and 12 male.
	
	\subsection{Logistics}
	
	Participants were allotted 45 minutes each for the study, although there was no expectation for the study to take over 20 minutes. Each participant began the experiment by reading an informed consent form which explained how the data would be collected, what steps were taken to ensure anonymity, and potential benefits from the study. After signing the form, each participant was given a quick overview of what they were expected to do: they were asked to use the iPad app to read 12 texts back to back, at their own pace, with an emphasis on reading for comprehension. They were given a chance to read a ``test passage," a passage of around 10 words meant to allow users to get used to the controls that had been preset for them. This was in order to avoid recording data that was in reality the user getting used to the given controls.
	
	After this test passage, the participant is asked to click the ``Next'' button which appears once a text is complete. For each of the subsequent 12 passages, the participant read the text in silence, and upon completion, summarizes the text briefly for the researcher out loud. We required each participant to summarize each text in order to ensure they were, in fact, reading the passage for comprehension, rather than speeding through the text. Each text the participants read has an ``A'' and a ``B'' version, and a participant reading the ``A'' version of a text does not read the ``B'', and vice versa. The differences between these two versions will be explained in Section 4.3.
	
	After reading all 12 passages, the participant was given \$5 as compensation, and asked to sign a form acknowledging receipt of this compensation. If requested, participants were also given a brief explanation of what the texts they read were meant to represent. This information was saved until after the experiment in order to avoid the participants knowing what to expect from the text, as we intended to capture as natural a reading process as possible.
	
	The data collected during the participant's reading consists of time values, acceleration values and text indices corresponding to the text on the screen at that time. This data is stored via email.
	
	\subsection{Text Pairs}
	
	The texts used in the experiment were constructed based on sentence pair comparison. The idea behind this technique is as follows: in an effort to identify which aspects of a text are ``easy'' or ``hard'' to read, sentence pairs are constructed, where one sentence serves as control (``B'') and the other as test (``A''). The two sentences differ from each other in small but significant ways. Asking multiple users to read version A, and asking multiple different users to read version B, allows us to compare whether, on average, the differences between A and B had a significant impact on the ``difficulty'' of that passage. Each of the 12 passages the participants read contained three such target sentences, giving us a total of 36 sentence pairs to compare.
	
	There are three kinds of differences that were being tested using the sentence pairs. With four texts for each kind of difference, and each text containing three target sentence, there are 12 sentence pairs per type of difference. These types are as follows.
	
	\textit{Lexical.} A lexical sentence pair consists of two sentences that are identical except for one word. These words must be synonyms of one another, where one of them (the B version) is expected to be more ``difficult'' than the other. An example is given below:
	\begin{itemize}[noitemsep, nolistsep]
		\item[A.] ``... believes that he escapes \textit{liability}... ''
		\item[B.] ``... believes that he escapes \textit{responsibility}... ''
	\end{itemize}
	
	\textit{Semantic.} A semantic sentence pair also consists of two sentences that are identical except for one word. These words are equally correct from a grammatical point of view, but one of them (the B version) is unexpected in the context of the passage, while the other (the A version) must be highly predictable. For instance:
	\begin{itemize}[noitemsep, nolistsep]
		\item[A.] ``... mailed her a \textit{compass} from China... ''
		\item[B.] ``... mailed her a \textit{letter} from China... ''
	\end{itemize}
	
	\textit{Syntactic.} A syntactic sentence pair consists of a garden path sentence (the B version) and another sentence (the A version) which means precisely the same thing as the garden path sentence, but with a clearer syntactic structure. A garden path sentence is one where a reader begins to apply a certain syntactic structure, only to reach a point in the sentence where the structure no longer applies. The addition of some qualifiers explains the intended syntactic structure. Consider the following:
	\begin{itemize}[noitemsep, nolistsep]
		\item[A.] ``The horse raced past the barn \textit{fell}.''
		\item[B.] ``The horse \textit{which was} raced past the barn fell.''
	\end{itemize}
	
	\section{Data Analysis}
	
	\subsection{Acquisition}
	
	Data analysis consisted of two stages, the first run while the app was operational, and the second post-testing. During the operation phase, the indices of the characters on screen were calculated every update step. At this time, the accelerometer reading was converted into characters per second using a linear factor which included the size of each character, the time since last update, and the size of the viewing window. Note that the size of each character was the same by design, since we chose to use a monospaced font to make such calculations easier.
	
	\subsection{Processing}
	
	After testing was completed, this data was further analyzed by a separate program. This program gathered all instances of words read per user (i.e., the words whose indices had been recorded) per text, and calculated the average characters per second of both the overall 12 texts, and the individual words within that text. Each individual word was then scored using the Z-Score of its average speed versus the average speed of the particular text. For instances where a word was truncated, a specific fraction had to be present for the word to be considered ``in-frame."
	
	
	Once all users' data had gone through this processing, the scores of users who had read the same texts were averaged, and a standard deviation for this new mean was found. This gave a normalized score per word across all users. Upon initial analysis of this data (see results), a new metric was implemented which assigned each text?s A and B versions a score given by averaging the three word scores at the ``target" words.
	
	
	\section{Results}
	
	Unfortunately, the results of the data analysis proved to be largely inconclusive. By comparing average Z-Scores of partner texts, as shown in figure [FIGURE NUMBER], we see that only four text pairs showed significant differences in reading speed, with $p < 0.05$. A higher Z-Score indicates a higher reading speed, thus three of the four significant pairs support the hypothesis, that the theoretically harder passages would have a lower reading speed across their three target sentences.
	
	\section{Summary and Future Directions}
	
	\subsection{Summary}
	
	x
	
	\subsection{Future Directions}
	
	x
	
	\section*{Acknowledgments}
	
	We are grateful to Professor Julie Medero for being our advisor during this process, and to the Harvey Mudd College CS Department for housing and supporting us. We would also like to thank the summer research students who participated in our studies.
	
	\begin{thebibliography}{}
		
		\bibitem[\protect\citename{{Huffington Post}}2013]{Huff:13}
		{Huffington Post}.
		\newblock 2013.
		\newblock {``The U.S. Illiteracy Rate Hasn't Changed in 10 Years.''}
		\newblock Huffington Post, Web.
		
		\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
		Alfred~V. Aho and Jeffrey~D. Ullman.
		\newblock 1972.
		\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
		\newblock Prentice-{Hall}, Englewood Cliffs, NJ.
		
		\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
		{American Psychological Association}.
		\newblock 1983.
		\newblock {\em Publications Manual}.
		\newblock American Psychological Association, Washington, DC.
		
		\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
		{Association for Computing Machinery}.
		\newblock 1983.
		\newblock {\em Computing Reviews}, 24(11):503--512.
		
		\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
		Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
		\newblock 1981.
		\newblock Alternation.
		\newblock {\em Journal of the Association for Computing Machinery},
		28(1):114--133.
		
		\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
		Dan Gusfield.
		\newblock 1997.
		\newblock {\em Algorithms on Strings, Trees and Sequences}.
		\newblock Cambridge University Press, Cambridge, UK.
		
	\end{thebibliography}
	
\end{document}
